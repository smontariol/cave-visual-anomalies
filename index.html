<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="CAVE - EMNLP 2025">
  <meta name="keywords" content="Visual anomalies, commonsense reasoning, vision-language models, multimodal benchmark">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CAVE - EMNLP 2025</title>

  <!-- Fonts and CSS -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="images/cave.png">
</head>

<body>
  <!-- Title Section -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div style="display: flex; align-items: center; justify-content: center;">
              <img src="images/cave.png" style="height: 100px; margin-right: 15px; margin-top: -10px;">
              <h1 class="title is-1 publication-title">CAVE</h1>
            </div>
            <h2>Detecting and Explaining <b>C</b>ommonsense <b>A</b>nomalies<br>in <b>V</b>isual <b>E</b>nvironments</h2>
            <h3 style="color:red;">EMNLP 2025 Main</h3>

            <div class="is-size-5">
              <span class="author-block"><b>Rishika Bhagwatkar</b><sup>1,2,*</sup>,</span>
              <span class="author-block"><b>Syrielle Montariol</b><sup>1,*</sup>,</span>
              <span class="author-block">Angelika Romanou<sup>1</sup>,</span>
              <span class="author-block">Beatriz Borges<sup>1</sup>,</span>
              <span class="author-block">Irina Rish<sup>2</sup>,</span>
              <span class="author-block">Antoine Bosselut<sup>1</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> École Polytechnique Fédérale de Lausanne (EPFL)</span>, 
              <span class="author-block"><sup>2</sup> Quebec Artificial Intelligence Institute (Mila)</span>
            </div>
            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup> Equal Contribution</span>
            </div>
            <!-- Links -->
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.xxxxx" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/<your-repo>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/<your-dataset>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><img src="images/hf-logo.png" style="width: 20px; height: 20px;"></span>
                    <span>Dataset</span>
                  </a>
                </span>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

<!-- Abstract / Summary -->
<section class="section" style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <div class="columns is-vcentered">

      <!-- Left column: Abstract text -->
      <div class="column is-half">
        <h2 class="title is-3">Summary</h2>
        <div class="content has-text-justified">
          <p>
            Humans can naturally identify, reason about, and explain anomalies in their environment.
            <br><br>
            We introduce <b>CAVE</b>, the first benchmark of real-world commonsense anomalies.
            <b>CAVE</b> contains images captured in real-world scenarios and supports three open-ended tasks:
            <i>anomaly description, explanation, and justification</i>.
            It also includes numerical features representing how humans perceive these anomalies:
            <b>complexity, severity, and commonness</b>.
            <br><br>
            These annotations draw inspiration from cognitive science research on how humans
            identify and resolve anomalies, providing a comprehensive framework for evaluating
            Vision-Language Models (VLMs) in detecting and understanding anomalies.
            We show that state-of-the-art VLMs struggle with visual anomaly perception
            and commonsense reasoning, even with advanced prompting strategies.
          </p>
        </div>
      </div>

      <!-- Right column: Figure -->
      <div class="column is-half has-text-centered">
        <img src="images/main.png" alt="CAVE Main Teaser" style="max-width:95%; border-radius:10px;">
        <p style="margin-top:10px;"><b>Figure:</b> Main overview of CAVE benchmark.</p>
      </div>

    </div>
  </div>
</section>

<!-- Figures / Benchmark Section -->
<section class="section" style="background-color:#f9fafc;">
  <div class="container is-max-desktop">

    <!-- Overview -->
    <div class="columns is-centered has-text-justified">
      <div class="column">
        <h2 class="title is-3">CAVE Benchmark Overview</h2>
        <p class="content is-medium">
          CAVE was created to evaluate how Vision-Language Models (VLMs) handle
          <b>commonsense anomalies</b> in real-world images. 
          <br>
          <b>(1) Image Collection:</b> Images were sourced from the top 1,000 posts across various subreddits and filtered to ensure high-quality and harmless data.<br>
          <b>(2) Human Annotation:</b> Initial annotations were performed by Mechanical Turk workers, focusing on anomaly identification and classification.<br>
          <b>(3) Expert Verification & Annotation:</b> A subsequent round of expert-driven annotation and verification ensured high-quality, consistent annotations across three open-ended tasks: <i>description, explanation, and justification</i>, as well as along numerical axes of <b>severity, surprisal</b>, and <b>complexity</b>.<br>
       </p>
      </div>
    </div>
    <div class="text-center">
      <img src="images/CAVE_process.png" width="100%" alt="CAVE Benchmark Overview">
      <p><b>Figure 1:</b> Overview of the CAVE data collection and annotation pipeline.</p>
    </div>

    <!-- Dataset Statistics -->
    <br><br>
    <section style="background-color:#eef7fb; padding:2em; border-radius:15px;">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h3 class="title is-3">Dataset Statistics</h3>
          <p class="content is-medium">
            The dataset includes <b>361 images</b> and <b>334 anomalies</b> across categories such as <i>entity presence/absence, attribute errors, spatial relation mismatches, 
            uniformity breaches,</i> and <i>textual anomalies</i>. Each image contains 0 to 3 anomalies. Each anomaly is paired with human-written descriptions, explanations, and plausible justifications, along with fine-grained scores:  
            <b>Severity</b> (impact or risk),  
            <b>Surprisal</b> (unexpectedness),
            <b>Complexity</b> (ease of detection).  
          </p>
        </div>
      </div>
      <div class="text-center">
        <img src="images/CAVE_statistics.png" width="100%" alt="CAVE Dataset Statistics">
        <p><b>Figure 2:</b> Statistics of anomaly categories and numerical attributes.</p>
      </div>
    </section>

    <!-- Experimental Results -->
    <br><br>
    <section style="background-color:#fff6ec; padding:2em; border-radius:15px;">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h3 class="title is-3">Experimental Results</h3>
          <p class="content is-medium">
            We evaluated <b>8 state-of-the-art VLMs</b> on CAVE.  
            Even the strongest model only achieved around <b>57% F1</b> in anomaly description.
            Models perform best on <i>severe and surprising anomalies</i>, but struggle with
            <i>complex perception</i>, especially spatial reasoning and pattern detection.
          </p>
        </div>
      </div>
      <div class="text-center">
        <img src="images/CAVE_results.png" width="100%" alt="CAVE Results">
        <p><b>Figure 3:</b> Performance of leading VLMs for anomaly detection (AD) and anomaly explanation (AE), evaluated using GPT4o-as-a-judge. We test various prompting strategies: Chain-of-Thought (CoT), Set-of-Marks (SoM), multi-steps reasoning (MS CoT), and self-consistency (CoT + consist.)</p>
      </div>
    </section>

    <!-- Examples -->
    <br><br>
    <section style="background-color:#f3f9f1; padding:2em; border-radius:15px;">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h3 class="title is-3">Example Anomalies</h3>
          <p class="content is-medium">
            CAVE captures a wide variety of <b>real-world commonsense anomalies</b>, ranging from 
            misplaced objects to textual inconsistencies.  
            This diversity challenges models to detect anomalies beyond simple object recognition and requires reasoning about <i>context and expectations</i>.
          </p>
        </div>
      </div>
      <div class="text-center">
        <img src="images/CAVE_examples.png" width="90%" alt="CAVE Example Images">
        <p><b>Figure 4:</b> Examples from CAVE showing diverse anomaly types.</p>
      </div>
    </section>

  </div>
</section>

  <!-- Takeaways -->
  <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Key Findings</h2>
      </div>
    </div>
    <div class="container is-max-desktop content">
      <ul>
        <li>State-of-the-art VLMs achieve only ~57% F1 on anomaly detection.</li>
        <li>Models perform better on severe/surprising anomalies, but struggle with complex reasoning tasks.</li>
        <li>Visual grounding and structured prompting improve recall modestly, but issues remain.</li>
        <li>Cultural and commonsense gaps highlight the need for more inclusive evaluation.</li>
      </ul>
    </div>
  </section>

  <!-- BibTeX Section -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@inproceedings{bhagwatkar2025cave,
  title={CAVE: Commonsense Anomalies in Visual Environments},
  author={Bhagwatkar, Rishika and Montariol, Syrielle and Romanou, Angelika and Borges, Beatriz and Rish, Irina and Bosselut, Antoine},
  booktitle={EMNLP 2025},
  year={2025}
}
      </code></pre>
    </div>
  </section>

  <!-- Contact -->
  <section class="section" id="contact">
    <div class="container is-max-desktop content">
      <h2 class="title">Contact</h2>
      <p>Rishika Bhagwatkar: <code>rishika.bhagwatkar@mila.quebec</code></p>
      <p>Syrielle Montariol: <code>syrielle.montariol@epfl.ch</code></p>
    </div>
  </section>

  <!-- Acknowledgement -->
  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a href="https://github.com/llava-vl/llava-vl.github.io">LLaVA-VL</a>,
        <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, and 
        <a href="https://vl-rewardbench.github.io/">VL-RewardBench</a>, licensed under
        <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.
      </p>
    </div>
  </section>
</body>
</html>